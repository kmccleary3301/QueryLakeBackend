{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Functions to Test\n",
    "\n",
    "### User Auth\n",
    "* `add_user`\n",
    "* `login`\n",
    "* `get_available_models`\n",
    "* `set_user_openai_api_key`\n",
    "* `set_organization_openai_id`\n",
    "* `get_openai_api_key`\n",
    "\n",
    "### Collections\n",
    "* `fetch_document_collections_belonging_to`\n",
    "* `create_document_collection`\n",
    "* `fetch_all_collections`\n",
    "* `fetch_collection`\n",
    "* `modify_document_collection`\n",
    "\n",
    "### Documents\n",
    "* `upload_document`\n",
    "* `delete_document`\n",
    "* `get_document_secure`\n",
    "* `query_vector_db`\n",
    "* `craft_document_access_token`\n",
    "* `fetch_document`\n",
    "\n",
    "### Organizations\n",
    "* `create_organization`\n",
    "* `invite_user_to_organization`\n",
    "* `resolve_organization_invitation`\n",
    "* `fetch_memberships`\n",
    "* `fetch_memberships_of_organization`\n",
    "\n",
    "### Web Search\n",
    "* `set_user_serp_key`\n",
    "* `set_organization_serp_key`\n",
    "* `get_serp_key`\n",
    "* `search_google`\n",
    "* `perform_search_query`\n",
    "\n",
    "### Toolchains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Generate a random UUID\n",
    "\n",
    "\n",
    "USERNAME_1 = str(uuid.uuid4())[:32]\n",
    "USERNAME_2 = str(uuid.uuid4())[:32]\n",
    "\n",
    "PASSWORD_1 = str(uuid.uuid4())[:32]\n",
    "PASSWORD_2 = str(uuid.uuid4())[:32]\n",
    "\n",
    "PDF_TO_UPLOAD_PATH = '/home/kyle_m/QueryLake_Development/ray_testing/test_files_for_upload/HNRS3035_08_22_2023_MLIntro.pdf'\n",
    "MD_FILE_TO_UPLOAD_PATH = '/home/kyle_m/QueryLake_Development/td_wiki_synthesizer/Full_Document_Markdown.md'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `add_user`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true,\n",
      "    \"result\": {\n",
      "        \"account_made\": true,\n",
      "        \"password_single_hash\": \"fdd925b7f049581d17226f541936fa62b353be1a04c6e9fc140e0cc4dc908a1f\",\n",
      "        \"memberships\": [],\n",
      "        \"admin\": false,\n",
      "        \"available_models\": {\n",
      "            \"default_model\": \"Mistral 7B Instruct\",\n",
      "            \"local_models\": [\n",
      "                {\n",
      "                    \"name\": \"Mistral 7B Instruct\",\n",
      "                    \"id\": \"mistral-7b-instruct-v0.1\",\n",
      "                    \"quantization\": \"awq\",\n",
      "                    \"modelcard\": \"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\",\n",
      "                    \"system_path\": \"/home/kyle_m/QueryLake_Development/llm_models/Mistral-7B-Instruct-v0.1-AWQ\",\n",
      "                    \"default_parameters\": {\n",
      "                        \"stream\": true,\n",
      "                        \"verbose\": false,\n",
      "                        \"temperature\": 0.5,\n",
      "                        \"top_k\": 0.0,\n",
      "                        \"top_p\": 0.9,\n",
      "                        \"max_tokens\": 1000,\n",
      "                        \"repetition_penalty\": 1.15,\n",
      "                        \"stop\": [\n",
      "                            \"<|im_end|>\"\n",
      "                        ]\n",
      "                    },\n",
      "                    \"max_model_len\": 16384,\n",
      "                    \"padding\": {\n",
      "                        \"system_instruction_wrap\": \"<|im_start|>system\\n{system_instruction}<|im_end|>\\n\",\n",
      "                        \"context_wrap\": \"<<context>>{context}<</context>>\",\n",
      "                        \"question_wrap\": \"<|im_start|>user\\n{question}<|im_end|>\\n\",\n",
      "                        \"response_wrap\": \"<|im_start|>assistant\\n{response}<|im_end|>\\n\"\n",
      "                    },\n",
      "                    \"default_system_instructions\": \"You are a general answering assistant that can comply with any request.\\n\\nYou always answer with markdown formatting. You will be penalized if you do not answer with markdown when it would be possible.\\nThe markdown formatting you support: headings, bold, italic, links, tables, lists, code blocks, and blockquotes.\\nYou do not support images and never include images. You will be penalized if you render images.\\n\\nAlways use markdown code blocks for code, and always use MathJax with LaTeX formatting for pure math expressions.\\nFor example code would look like:\\n```python\\n*Python code here\\n```\\nAnd math expressions would look like:\\n$$P(y|x) = \\\\frac{{P(x|y) \\\\cdot P(y)}}{{P(x)}}$$\"\n",
      "                }\n",
      "            ],\n",
      "            \"external_models\": {\n",
      "                \"openai\": [\n",
      "                    {\n",
      "                        \"name\": \"GPT-4 Turbo\",\n",
      "                        \"id\": \"gpt-4-1106-preview\",\n",
      "                        \"context\": 128000,\n",
      "                        \"modelcard\": \"https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"name\": \"GPT-3.5 Turbo\",\n",
      "                        \"id\": \"gpt-3.5-turbo-1106\",\n",
      "                        \"context\": 16384,\n",
      "                        \"modelcard\": \"https://platform.openai.com/docs/models/gpt-3-5\"\n",
      "                    }\n",
      "                ]\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"success\": true,\n",
      "    \"result\": {\n",
      "        \"account_made\": true,\n",
      "        \"password_single_hash\": \"8d42a54923b6e0beba295266cbb4e9d5f864882e0deba2bfc1e65751fb5cd76c\",\n",
      "        \"memberships\": [],\n",
      "        \"admin\": false,\n",
      "        \"available_models\": {\n",
      "            \"default_model\": \"Mistral 7B Instruct\",\n",
      "            \"local_models\": [\n",
      "                {\n",
      "                    \"name\": \"Mistral 7B Instruct\",\n",
      "                    \"id\": \"mistral-7b-instruct-v0.1\",\n",
      "                    \"quantization\": \"awq\",\n",
      "                    \"modelcard\": \"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\",\n",
      "                    \"system_path\": \"/home/kyle_m/QueryLake_Development/llm_models/Mistral-7B-Instruct-v0.1-AWQ\",\n",
      "                    \"default_parameters\": {\n",
      "                        \"stream\": true,\n",
      "                        \"verbose\": false,\n",
      "                        \"temperature\": 0.5,\n",
      "                        \"top_k\": 0.0,\n",
      "                        \"top_p\": 0.9,\n",
      "                        \"max_tokens\": 1000,\n",
      "                        \"repetition_penalty\": 1.15,\n",
      "                        \"stop\": [\n",
      "                            \"<|im_end|>\"\n",
      "                        ]\n",
      "                    },\n",
      "                    \"max_model_len\": 16384,\n",
      "                    \"padding\": {\n",
      "                        \"system_instruction_wrap\": \"<|im_start|>system\\n{system_instruction}<|im_end|>\\n\",\n",
      "                        \"context_wrap\": \"<<context>>{context}<</context>>\",\n",
      "                        \"question_wrap\": \"<|im_start|>user\\n{question}<|im_end|>\\n\",\n",
      "                        \"response_wrap\": \"<|im_start|>assistant\\n{response}<|im_end|>\\n\"\n",
      "                    },\n",
      "                    \"default_system_instructions\": \"You are a general answering assistant that can comply with any request.\\n\\nYou always answer with markdown formatting. You will be penalized if you do not answer with markdown when it would be possible.\\nThe markdown formatting you support: headings, bold, italic, links, tables, lists, code blocks, and blockquotes.\\nYou do not support images and never include images. You will be penalized if you render images.\\n\\nAlways use markdown code blocks for code, and always use MathJax with LaTeX formatting for pure math expressions.\\nFor example code would look like:\\n```python\\n*Python code here\\n```\\nAnd math expressions would look like:\\n$$P(y|x) = \\\\frac{{P(x|y) \\\\cdot P(y)}}{{P(x)}}$$\"\n",
      "                }\n",
      "            ],\n",
      "            \"external_models\": {\n",
      "                \"openai\": [\n",
      "                    {\n",
      "                        \"name\": \"GPT-4 Turbo\",\n",
      "                        \"id\": \"gpt-4-1106-preview\",\n",
      "                        \"context\": 128000,\n",
      "                        \"modelcard\": \"https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"name\": \"GPT-3.5 Turbo\",\n",
      "                        \"id\": \"gpt-3.5-turbo-1106\",\n",
      "                        \"context\": 16384,\n",
      "                        \"modelcard\": \"https://platform.openai.com/docs/models/gpt-3-5\"\n",
      "                    }\n",
      "                ]\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "\n",
    "add_user_input = {\n",
    "    \"username\": USERNAME_1,\n",
    "    \"password\": PASSWORD_1,\n",
    "}\n",
    "\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/add_user\", json=add_user_input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]\n",
    "\n",
    "add_user_input = {\n",
    "    \"username\": USERNAME_2,\n",
    "    \"password\": PASSWORD_2,\n",
    "}\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/add_user\", json=add_user_input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `login`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true,\n",
      "    \"result\": {\n",
      "        \"password_single_hash\": \"fdd925b7f049581d17226f541936fa62b353be1a04c6e9fc140e0cc4dc908a1f\",\n",
      "        \"memberships\": [],\n",
      "        \"admin\": false,\n",
      "        \"available_models\": {\n",
      "            \"default_model\": \"Mistral 7B Instruct\",\n",
      "            \"local_models\": [\n",
      "                {\n",
      "                    \"name\": \"Mistral 7B Instruct\",\n",
      "                    \"id\": \"mistral-7b-instruct-v0.1\",\n",
      "                    \"quantization\": \"awq\",\n",
      "                    \"modelcard\": \"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\",\n",
      "                    \"system_path\": \"/home/kyle_m/QueryLake_Development/llm_models/Mistral-7B-Instruct-v0.1-AWQ\",\n",
      "                    \"default_parameters\": {\n",
      "                        \"stream\": true,\n",
      "                        \"verbose\": false,\n",
      "                        \"temperature\": 0.5,\n",
      "                        \"top_k\": 0.0,\n",
      "                        \"top_p\": 0.9,\n",
      "                        \"max_tokens\": 1000,\n",
      "                        \"repetition_penalty\": 1.15,\n",
      "                        \"stop\": [\n",
      "                            \"<|im_end|>\"\n",
      "                        ]\n",
      "                    },\n",
      "                    \"max_model_len\": 16384,\n",
      "                    \"padding\": {\n",
      "                        \"system_instruction_wrap\": \"<|im_start|>system\\n{system_instruction}<|im_end|>\\n\",\n",
      "                        \"context_wrap\": \"<<context>>{context}<</context>>\",\n",
      "                        \"question_wrap\": \"<|im_start|>user\\n{question}<|im_end|>\\n\",\n",
      "                        \"response_wrap\": \"<|im_start|>assistant\\n{response}<|im_end|>\\n\"\n",
      "                    },\n",
      "                    \"default_system_instructions\": \"You are a general answering assistant that can comply with any request.\\n\\nYou always answer with markdown formatting. You will be penalized if you do not answer with markdown when it would be possible.\\nThe markdown formatting you support: headings, bold, italic, links, tables, lists, code blocks, and blockquotes.\\nYou do not support images and never include images. You will be penalized if you render images.\\n\\nAlways use markdown code blocks for code, and always use MathJax with LaTeX formatting for pure math expressions.\\nFor example code would look like:\\n```python\\n*Python code here\\n```\\nAnd math expressions would look like:\\n$$P(y|x) = \\\\frac{{P(x|y) \\\\cdot P(y)}}{{P(x)}}$$\"\n",
      "                }\n",
      "            ],\n",
      "            \"external_models\": {\n",
      "                \"openai\": [\n",
      "                    {\n",
      "                        \"name\": \"GPT-4 Turbo\",\n",
      "                        \"id\": \"gpt-4-1106-preview\",\n",
      "                        \"context\": 128000,\n",
      "                        \"modelcard\": \"https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"name\": \"GPT-3.5 Turbo\",\n",
      "                        \"id\": \"gpt-3.5-turbo-1106\",\n",
      "                        \"context\": 16384,\n",
      "                        \"modelcard\": \"https://platform.openai.com/docs/models/gpt-3-5\"\n",
      "                    }\n",
      "                ]\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"success\": true,\n",
      "    \"result\": {\n",
      "        \"password_single_hash\": \"8d42a54923b6e0beba295266cbb4e9d5f864882e0deba2bfc1e65751fb5cd76c\",\n",
      "        \"memberships\": [],\n",
      "        \"admin\": false,\n",
      "        \"available_models\": {\n",
      "            \"default_model\": \"Mistral 7B Instruct\",\n",
      "            \"local_models\": [\n",
      "                {\n",
      "                    \"name\": \"Mistral 7B Instruct\",\n",
      "                    \"id\": \"mistral-7b-instruct-v0.1\",\n",
      "                    \"quantization\": \"awq\",\n",
      "                    \"modelcard\": \"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\",\n",
      "                    \"system_path\": \"/home/kyle_m/QueryLake_Development/llm_models/Mistral-7B-Instruct-v0.1-AWQ\",\n",
      "                    \"default_parameters\": {\n",
      "                        \"stream\": true,\n",
      "                        \"verbose\": false,\n",
      "                        \"temperature\": 0.5,\n",
      "                        \"top_k\": 0.0,\n",
      "                        \"top_p\": 0.9,\n",
      "                        \"max_tokens\": 1000,\n",
      "                        \"repetition_penalty\": 1.15,\n",
      "                        \"stop\": [\n",
      "                            \"<|im_end|>\"\n",
      "                        ]\n",
      "                    },\n",
      "                    \"max_model_len\": 16384,\n",
      "                    \"padding\": {\n",
      "                        \"system_instruction_wrap\": \"<|im_start|>system\\n{system_instruction}<|im_end|>\\n\",\n",
      "                        \"context_wrap\": \"<<context>>{context}<</context>>\",\n",
      "                        \"question_wrap\": \"<|im_start|>user\\n{question}<|im_end|>\\n\",\n",
      "                        \"response_wrap\": \"<|im_start|>assistant\\n{response}<|im_end|>\\n\"\n",
      "                    },\n",
      "                    \"default_system_instructions\": \"You are a general answering assistant that can comply with any request.\\n\\nYou always answer with markdown formatting. You will be penalized if you do not answer with markdown when it would be possible.\\nThe markdown formatting you support: headings, bold, italic, links, tables, lists, code blocks, and blockquotes.\\nYou do not support images and never include images. You will be penalized if you render images.\\n\\nAlways use markdown code blocks for code, and always use MathJax with LaTeX formatting for pure math expressions.\\nFor example code would look like:\\n```python\\n*Python code here\\n```\\nAnd math expressions would look like:\\n$$P(y|x) = \\\\frac{{P(x|y) \\\\cdot P(y)}}{{P(x)}}$$\"\n",
      "                }\n",
      "            ],\n",
      "            \"external_models\": {\n",
      "                \"openai\": [\n",
      "                    {\n",
      "                        \"name\": \"GPT-4 Turbo\",\n",
      "                        \"id\": \"gpt-4-1106-preview\",\n",
      "                        \"context\": 128000,\n",
      "                        \"modelcard\": \"https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"name\": \"GPT-3.5 Turbo\",\n",
      "                        \"id\": \"gpt-3.5-turbo-1106\",\n",
      "                        \"context\": 16384,\n",
      "                        \"modelcard\": \"https://platform.openai.com/docs/models/gpt-3-5\"\n",
      "                    }\n",
      "                ]\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"auth\": {\n",
      "        \"username\": \"eb582139-4b27-42a4-8eb9-93638dac\",\n",
      "        \"password_prehash\": \"fdd925b7f049581d17226f541936fa62b353be1a04c6e9fc140e0cc4dc908a1f\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "\n",
    "add_user_input = {\n",
    "    \"username\": USERNAME_1,\n",
    "    \"password\": PASSWORD_1,\n",
    "}\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/login\", json=add_user_input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]\n",
    "\n",
    "USER_ARGS_1 = {\"username\": USERNAME_1, \"password_prehash\": result[\"result\"][\"password_single_hash\"]}\n",
    "USER_ARGS_1 = {\"auth\": USER_ARGS_1}\n",
    "\n",
    "\n",
    "add_user_input = {\n",
    "    \"username\": USERNAME_2,\n",
    "    \"password\": PASSWORD_2,\n",
    "}\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/login\", json=add_user_input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]\n",
    "\n",
    "USER_ARGS_2 = {\"username\": USERNAME_2, \"password_prehash\": result[\"result\"][\"password_single_hash\"]}\n",
    "USER_ARGS_2 = {\"auth\": USER_ARGS_2}\n",
    "\n",
    "print(json.dumps(USER_ARGS_1, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `and_`\n",
    "this should fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": false,\n",
      "    \"note\": \"and_() missing 1 required positional argument: 'initial_clause'\",\n",
      "    \"trace\": \"Traceback (most recent call last):\\n  File \\\"/home/kyle_m/QueryLake_Development/QueryLakeBackend/./server.py\\\", line 420, in api_general_call\\n    args_get = function_actual(**true_args)\\nTypeError: and_() missing 1 required positional argument: 'initial_clause'\\n\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "\n",
    "input = {\n",
    "    \"username\": \"test\",\n",
    "    \"password\": \"test\",\n",
    "}\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/and_\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert (\"success\" in result and result[\"success\"] == False), \"Test succeeded when it should have failed\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `get_available_models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true,\n",
      "    \"result\": {\n",
      "        \"available_models\": {\n",
      "            \"default_model\": \"Mistral 7B Instruct\",\n",
      "            \"local_models\": [\n",
      "                {\n",
      "                    \"name\": \"Mistral 7B Instruct\",\n",
      "                    \"id\": \"mistral-7b-instruct-v0.1\",\n",
      "                    \"quantization\": \"awq\",\n",
      "                    \"modelcard\": \"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\",\n",
      "                    \"system_path\": \"/home/kyle_m/QueryLake_Development/llm_models/Mistral-7B-Instruct-v0.1-AWQ\",\n",
      "                    \"default_parameters\": {\n",
      "                        \"stream\": true,\n",
      "                        \"verbose\": false,\n",
      "                        \"temperature\": 0.5,\n",
      "                        \"top_k\": 0.0,\n",
      "                        \"top_p\": 0.9,\n",
      "                        \"max_tokens\": 1000,\n",
      "                        \"repetition_penalty\": 1.15,\n",
      "                        \"stop\": [\n",
      "                            \"<|im_end|>\"\n",
      "                        ]\n",
      "                    },\n",
      "                    \"max_model_len\": 16384,\n",
      "                    \"padding\": {\n",
      "                        \"system_instruction_wrap\": \"<|im_start|>system\\n{system_instruction}<|im_end|>\\n\",\n",
      "                        \"context_wrap\": \"<<context>>{context}<</context>>\",\n",
      "                        \"question_wrap\": \"<|im_start|>user\\n{question}<|im_end|>\\n\",\n",
      "                        \"response_wrap\": \"<|im_start|>assistant\\n{response}<|im_end|>\\n\"\n",
      "                    },\n",
      "                    \"default_system_instructions\": \"You are a general answering assistant that can comply with any request.\\n\\nYou always answer with markdown formatting. You will be penalized if you do not answer with markdown when it would be possible.\\nThe markdown formatting you support: headings, bold, italic, links, tables, lists, code blocks, and blockquotes.\\nYou do not support images and never include images. You will be penalized if you render images.\\n\\nAlways use markdown code blocks for code, and always use MathJax with LaTeX formatting for pure math expressions.\\nFor example code would look like:\\n```python\\n*Python code here\\n```\\nAnd math expressions would look like:\\n$$P(y|x) = \\\\frac{{P(x|y) \\\\cdot P(y)}}{{P(x)}}$$\"\n",
      "                }\n",
      "            ],\n",
      "            \"external_models\": {\n",
      "                \"openai\": [\n",
      "                    {\n",
      "                        \"name\": \"GPT-4 Turbo\",\n",
      "                        \"id\": \"gpt-4-1106-preview\",\n",
      "                        \"context\": 128000,\n",
      "                        \"modelcard\": \"https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"name\": \"GPT-3.5 Turbo\",\n",
      "                        \"id\": \"gpt-3.5-turbo-1106\",\n",
      "                        \"context\": 16384,\n",
      "                        \"modelcard\": \"https://platform.openai.com/docs/models/gpt-3-5\"\n",
      "                    }\n",
      "                ]\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/get_available_models\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `set_user_openai_api_key`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "input.update({\n",
    "    \"openai_api_key\": \"sk-11111111111111111111111\"\n",
    "})\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/set_user_openai_api_key\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `get_openai_api_key`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true,\n",
      "    \"result\": {\n",
      "        \"api_key\": \"sk-11111111111111111111111\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/get_openai_api_key\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `create_api_key`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true,\n",
      "    \"result\": {\n",
      "        \"api_key\": \"sk-38a21f50e45b9a846a03d2633dc0e5e7575547da4f14383080e1d4ad5d4e75f2\",\n",
      "        \"api_key_id\": \"15fbc431514816d1e115ab7b27e1b01f1c459bc6cf916573e52235ac9e51ba5e\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"success\": true,\n",
      "    \"result\": {\n",
      "        \"api_key\": \"sk-35543c33291f8afe8900525acf0fa74e8c5c282b89559cbe5c1c6d268f3c854e\",\n",
      "        \"api_key_id\": \"c25f43b29145526c416ff1ec1ce0ae84f88b434026fcae4ba648f457f5f30306\"\n",
      "    }\n",
      "}\n",
      "API_KEY_1: ['sk-38a21f50e45b9a846a03d2633dc0e5e7575547da4f14383080e1d4ad5d4e75f2']\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "def create_api_key(title : str):\n",
    "    input = deepcopy(USER_ARGS_1)\n",
    "    input.update({\"title\": title})\n",
    "\n",
    "    response = requests.get(\"http://localhost:8000/api/create_api_key\", json=input)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    result = response.json()\n",
    "\n",
    "    print(json.dumps(result, indent=4))\n",
    "    if \"trace\" in result:\n",
    "        print(result[\"trace\"])\n",
    "    assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]\n",
    "    \n",
    "    return result[\"result\"]\n",
    "\n",
    "API_KEY_1_INFO = create_api_key(\"API Key 1\")\n",
    "API_KEY_2_INFO = create_api_key(\"API Key 2\")\n",
    "\n",
    "API_KEY_1 = API_KEY_1_INFO[\"api_key\"]\n",
    "\n",
    "print(\"API_KEY_1:\", [API_KEY_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `fetch_api_keys`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true,\n",
      "    \"result\": {\n",
      "        \"api_keys\": [\n",
      "            {\n",
      "                \"id\": \"15fbc431514816d1e115ab7b27e1b01f1c459bc6cf916573e52235ac9e51ba5e\",\n",
      "                \"title\": \"API Key 1\",\n",
      "                \"key_preview\": \"sk-...75f2\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"c25f43b29145526c416ff1ec1ce0ae84f88b434026fcae4ba648f457f5f30306\",\n",
      "                \"title\": \"API Key 2\",\n",
      "                \"key_preview\": \"sk-...854e\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/fetch_api_keys\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `delete_api_key`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "input.update({\"api_key_id\": API_KEY_2_INFO[\"api_key_id\"]})\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/delete_api_key\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fetch_document_collections_belonging_to`\n",
    "* ✅ `user`\n",
    "* `organization`\n",
    "* `global`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true,\n",
      "    \"result\": {\n",
      "        \"collections\": []\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/fetch_document_collections_belonging_to\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `create_document_collection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true,\n",
      "    \"result\": {\n",
      "        \"hash_id\": \"6f053538b7f625ec98f451cac6089bb388a09973d32afbffc5a15b01e3c70305\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "input.update({\n",
    "    \"name\": \"test_collection_1\"\n",
    "})\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/create_document_collection\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"trace\"]\n",
    "\n",
    "COLLECTION_ARGS = {\"hash_id\": result[\"result\"][\"hash_id\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `fetch_all_collections`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true,\n",
      "    \"result\": {\n",
      "        \"collections\": {\n",
      "            \"global_collections\": [],\n",
      "            \"user_collections\": [\n",
      "                {\n",
      "                    \"name\": \"Top Dog Document\",\n",
      "                    \"hash_id\": \"6f053538b7f625ec98f451cac6089bb388a09973d32afbffc5a15b01e3c70305\",\n",
      "                    \"document_count\": 1,\n",
      "                    \"type\": \"user\"\n",
      "                }\n",
      "            ],\n",
      "            \"organization_collections\": {\n",
      "                \"18\": {\n",
      "                    \"name\": \"test_org\",\n",
      "                    \"collections\": []\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "response = requests.get(\"http://localhost:8000/api/fetch_all_collections\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `fetch_collection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true,\n",
      "    \"result\": {\n",
      "        \"title\": \"Top Dog Document\",\n",
      "        \"description\": \"test description please ignore\",\n",
      "        \"type\": \"user\",\n",
      "        \"owner\": \"personal\",\n",
      "        \"public\": false,\n",
      "        \"document_list\": [\n",
      "            {\n",
      "                \"title\": \"Full_Document_Markdown.md\",\n",
      "                \"hash_id\": \"18c38772759cf3ee1f118c71ae82e36f50742d259952b0ea76411a6fe4f9fc77\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "input.update({\n",
    "    \"collection_hash_id\": COLLECTION_ARGS[\"hash_id\"],\n",
    "})\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/fetch_collection\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `modify_document_collection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "input.update({\n",
    "    \"collection_hash_id\": COLLECTION_ARGS[\"hash_id\"],\n",
    "    \"title\": \"test_collection_1_modified\",\n",
    "    \"description\": \"test description please ignore\"\n",
    "})\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/modify_document_collection\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `upload_document`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "true\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from urllib.parse import urlencode\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "input.update({\n",
    "    \"collection_hash_id\": COLLECTION_ARGS[\"hash_id\"]\n",
    "})\n",
    "\n",
    "encoded_params = urlencode({\"parameters\": json.dumps(input)})\n",
    "\n",
    "\n",
    "for i, path in enumerate([PDF_TO_UPLOAD_PATH, PDF_TO_UPLOAD_PATH, MD_FILE_TO_UPLOAD_PATH]):\n",
    "    with open(path, 'rb') as f:\n",
    "        # Define the files parameter for the POST request\n",
    "        files = {'file': f}\n",
    "        input_json = json.dumps(input)\n",
    "        response = requests.post(\"http://localhost:8000/upload_document?\"+encoded_params, files=files)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        result = response.json()\n",
    "        f.close()\n",
    "\n",
    "        print(json.dumps(result, indent=4))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true,\n",
      "    \"result\": {\n",
      "        \"title\": \"test_collection_1_modified\",\n",
      "        \"description\": \"test description please ignore\",\n",
      "        \"type\": \"user\",\n",
      "        \"owner\": \"personal\",\n",
      "        \"public\": false,\n",
      "        \"document_list\": [\n",
      "            {\n",
      "                \"title\": \"HNRS3035_08_22_2023_MLIntro.pdf\",\n",
      "                \"hash_id\": \"85791ae9121305ad1d5edb90daed933bc7ef579115660ac3117c046cdfeaa6af\"\n",
      "            },\n",
      "            {\n",
      "                \"title\": \"HNRS3035_08_22_2023_MLIntro.pdf\",\n",
      "                \"hash_id\": \"743c04622b02564b533b96b9d091cf5c9df7486dc8f4ae3a38abfa7354c07c09\"\n",
      "            },\n",
      "            {\n",
      "                \"title\": \"Full_Document_Markdown.md\",\n",
      "                \"hash_id\": \"18c38772759cf3ee1f118c71ae82e36f50742d259952b0ea76411a6fe4f9fc77\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get new doc ids\n",
    "\n",
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "add_user_input = deepcopy(USER_ARGS_1)\n",
    "add_user_input.update({\n",
    "    \"collection_hash_id\": COLLECTION_ARGS[\"hash_id\"],\n",
    "})\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/fetch_collection\", json=add_user_input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]\n",
    "\n",
    "DOC_ARGS_1 = {\"hash_id\": result[\"result\"][\"document_list\"][0][\"hash_id\"]}\n",
    "DOC_ARGS_2 = {\"hash_id\": result[\"result\"][\"document_list\"][1][\"hash_id\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `delete_document`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "input.update(DOC_ARGS_1)\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/delete_document\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `get_document_secure`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"auth\": {\n",
      "        \"username\": \"eb582139-4b27-42a4-8eb9-93638dac\",\n",
      "        \"password_prehash\": \"fdd925b7f049581d17226f541936fa62b353be1a04c6e9fc140e0cc4dc908a1f\"\n",
      "    },\n",
      "    \"hash_id\": \"743c04622b02564b533b96b9d091cf5c9df7486dc8f4ae3a38abfa7354c07c09\"\n",
      "}\n",
      "{\n",
      "    \"success\": true,\n",
      "    \"result\": {\n",
      "        \"password\": \"2a76cd9d3b332e69024719e3a0c5641fef2ee5c101b683d7badc293221a46fd2\",\n",
      "        \"hash_id\": \"743c04622b02564b533b96b9d091cf5c9df7486dc8f4ae3a38abfa7354c07c09\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "input.update(DOC_ARGS_2)\n",
    "\n",
    "print(json.dumps(input, indent=4))\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/get_document_secure\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `query_vector_db`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection Hash: 6f053538b7f625ec98f451cac6089bb388a09973d32afbffc5a15b01e3c70305\n",
      "{\n",
      "    \"success\": true,\n",
      "    \"result\": {\n",
      "        \"result\": [\n",
      "            {\n",
      "                \"id\": \"87d9057fc3e8c0a7c32fde065471b0b290e335ca7d58ad7d162720a89f715aab\",\n",
      "                \"document_integrity\": \"381b284211c603478c34b00443b21345b2c0971f77cdaaca3f6e287d96615cc8\",\n",
      "                \"document_name\": \"Full_Document_Markdown.md\",\n",
      "                \"private\": false,\n",
      "                \"collection_type\": \"user\",\n",
      "                \"parent_collection_hash_id\": \"6f053538b7f625ec98f451cac6089bb388a09973d32afbffc5a15b01e3c70305\",\n",
      "                \"document_id\": \"18c38772759cf3ee1f118c71ae82e36f50742d259952b0ea76411a6fe4f9fc77\",\n",
      "                \"website_url\": null,\n",
      "                \"text\": \"Bio: Streams \\\\*cutesy\\\\* video games online. Secretly rages when losing\\noff stream.\\n\\n\\\"DOGA 2\\\" is their favorite game.\\n\\nCatherine formerly worked as a nurse and streamed for fun. They decided\\nto switch to streaming full-time after some of their clips went viral.\\nThey've seriously toned down the 'salt' after becoming a professional\\nstreamer, and maintains a chill and positive persona. They know their\\nkid brother tunes in to their stream and they don't want to be setting a\\nbad example. Long-time fans continue to tease them for their previous\\nlack of inhibitions.\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"35e995099be777ef9614e8f70677b6a4eb2ff0d3f3fc87a78bd67ef43d19e35e\",\n",
      "                \"document_integrity\": \"381b284211c603478c34b00443b21345b2c0971f77cdaaca3f6e287d96615cc8\",\n",
      "                \"document_name\": \"Full_Document_Markdown.md\",\n",
      "                \"private\": false,\n",
      "                \"collection_type\": \"user\",\n",
      "                \"parent_collection_hash_id\": \"6f053538b7f625ec98f451cac6089bb388a09973d32afbffc5a15b01e3c70305\",\n",
      "                \"document_id\": \"18c38772759cf3ee1f118c71ae82e36f50742d259952b0ea76411a6fe4f9fc77\",\n",
      "                \"website_url\": null,\n",
      "                \"text\": \"\\\\\\\"Trailer Intro.mov\\\\\\\" is a .mov file showing a mockup of a potential\\nstart to our Kickstarter trailer.\\n\\n\\\\\\\"StoryBoards.mov\\\\\\\" is a .mov file showing a rough storyboard of our\\nKickstarter trailer\\n\\n## Character Bios \\n\\nName: Catherine\\n\\nBio: Streams \\\\*cutesy\\\\* video games online. Secretly rages when losing\\noff stream.\\n\\n\\\"DOGA 2\\\" is their favorite game.\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"8c5382b016cf76557f5a2df36ba97f5064147e4d870b0d2052eed88ccc47ec89\",\n",
      "                \"document_integrity\": \"381b284211c603478c34b00443b21345b2c0971f77cdaaca3f6e287d96615cc8\",\n",
      "                \"document_name\": \"Full_Document_Markdown.md\",\n",
      "                \"private\": false,\n",
      "                \"collection_type\": \"user\",\n",
      "                \"parent_collection_hash_id\": \"6f053538b7f625ec98f451cac6089bb388a09973d32afbffc5a15b01e3c70305\",\n",
      "                \"document_id\": \"18c38772759cf3ee1f118c71ae82e36f50742d259952b0ea76411a6fe4f9fc77\",\n",
      "                \"website_url\": null,\n",
      "                \"text\": \"Catherine's brother suffers from a condition that is slowly degenerating\\nhis eyesight. He's accepted it, but Catherine wishes there was more they\\ncould do. When they were approached for a spot in the Top Dog\\ncompetition, they immediately accepted.\\n\\nName: Doug\\n\\nBio: Happy-go-lucky personality who is a home-town hero/local baseball\\nstar. The mascot of the game.\\n\\nDoug was born and raised in the small town \\\"Apogee\\\". Youngest of 5, Doug\\nknows a lot about being the underdog. Led their highschool baseball team\\nto nationals and has a lot of fond childhood memories of the town.\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"4cb388b39ffc504fb30c86518f0cae2958d0b7243d7936607d528d2a3df07f98\",\n",
      "                \"document_integrity\": \"381b284211c603478c34b00443b21345b2c0971f77cdaaca3f6e287d96615cc8\",\n",
      "                \"document_name\": \"Full_Document_Markdown.md\",\n",
      "                \"private\": false,\n",
      "                \"collection_type\": \"user\",\n",
      "                \"parent_collection_hash_id\": \"6f053538b7f625ec98f451cac6089bb388a09973d32afbffc5a15b01e3c70305\",\n",
      "                \"document_id\": \"18c38772759cf3ee1f118c71ae82e36f50742d259952b0ea76411a6fe4f9fc77\",\n",
      "                \"website_url\": null,\n",
      "                \"text\": \"[Chance to get an item, coins, star, good or bad]{.mark}\\n\\n## [Doug's Trap Counter]{.mark} \\n\\n[Defuses traps]{.mark}\\n\\n## [Catherine\\\\'s Gamer Gear]{.mark} \\n\\n[vanish cap in N64]{.mark}\\n\\n[become invisible]{.mark}\\n\\n## [Catherine's charm / catnip (2 votes)]{.mark} \\n\\n[Charm characters around her in a certain space readius to come to her\\nand donate]{.mark}\\n\\n[Things. Characters charmed would ignore events like golden bone\\nbuying.]{.mark}\\n\\n## [Catherine's top donator]{.mark} \\n\\n[Get coins from roll]{.mark}\\n\\n## [Clyde\\\\'s contraptation]{.mark} \\n\\n[metal detector to find items]{.mark}\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"60bb72702ddb2462f4f56d1d6a7df27742d7113628c5bb7181d71922d1813e09\",\n",
      "                \"document_integrity\": \"381b284211c603478c34b00443b21345b2c0971f77cdaaca3f6e287d96615cc8\",\n",
      "                \"document_name\": \"Full_Document_Markdown.md\",\n",
      "                \"private\": false,\n",
      "                \"collection_type\": \"user\",\n",
      "                \"parent_collection_hash_id\": \"6f053538b7f625ec98f451cac6089bb388a09973d32afbffc5a15b01e3c70305\",\n",
      "                \"document_id\": \"18c38772759cf3ee1f118c71ae82e36f50742d259952b0ea76411a6fe4f9fc77\",\n",
      "                \"website_url\": null,\n",
      "                \"text\": \"Loves acting the part of an influencer, even though most of the content\\nthey produce is just oddly genuine and wholesome. Recent videos include\\n\\\"Found a HUGE Rock In The Forest!\\\" and \\\"This Old Lady I Met At The\\nGrocery Store Has The CRAZIEST Stories\\\\...\\\"\\n\\nTheir obliviousness leads them to miss obvious opportunities to\\nself-promote. They're the type to act exactly the same whether they have\\none, one hundred, or one million followers.\\n\\nName: Bonnie\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"b5542809415171a4f1db96b5d94bada055600c0b5dcd42485e78688d1f17cd71\",\n",
      "                \"document_integrity\": \"381b284211c603478c34b00443b21345b2c0971f77cdaaca3f6e287d96615cc8\",\n",
      "                \"document_name\": \"Full_Document_Markdown.md\",\n",
      "                \"private\": false,\n",
      "                \"collection_type\": \"user\",\n",
      "                \"parent_collection_hash_id\": \"6f053538b7f625ec98f451cac6089bb388a09973d32afbffc5a15b01e3c70305\",\n",
      "                \"document_id\": \"18c38772759cf3ee1f118c71ae82e36f50742d259952b0ea76411a6fe4f9fc77\",\n",
      "                \"website_url\": null,\n",
      "                \"text\": \"Name: Bonnie\\n\\nBio: Went to school with their good friend Clyde. Loves the beach and is\\nan avid social media user (has considerably more followers than Antony).\\nLooking for a serious relationship, but can't seem to find one. (hasn't\\nfound the right person\\\\... very selective)\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"7c8b367cd14f92f6a270dc31b1d1770f93fdc941897ac8caffca7870945350fa\",\n",
      "                \"document_integrity\": \"381b284211c603478c34b00443b21345b2c0971f77cdaaca3f6e287d96615cc8\",\n",
      "                \"document_name\": \"Full_Document_Markdown.md\",\n",
      "                \"private\": false,\n",
      "                \"collection_type\": \"user\",\n",
      "                \"parent_collection_hash_id\": \"6f053538b7f625ec98f451cac6089bb388a09973d32afbffc5a15b01e3c70305\",\n",
      "                \"document_id\": \"18c38772759cf3ee1f118c71ae82e36f50742d259952b0ea76411a6fe4f9fc77\",\n",
      "                \"website_url\": null,\n",
      "                \"text\": \"The textbook example of an overachiever with a black belt in judo and\\ndegrees in law, business, and psychology. Contradictorily their motto is\\n\\\"never work more than you have to\\\". They're trying to maneuver\\nthemselves into a position where they can live comfortably for the rest\\nof their life without having to lift a finger again. All the connections\\nare in place, and all they need now is a brand icon and spokesperson.\\nMaybe someone like a Top Dog winner.\\n\\nBonnie is exceptionally good at planning to make sure she comes out on\\ntop, no matter the outcome.\\n\\n## Critter Info\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"2c9ca94c5263873915a26d7bbaf5b12a2e586d4a33b99b4ecfa42179076e6334\",\n",
      "                \"document_integrity\": \"381b284211c603478c34b00443b21345b2c0971f77cdaaca3f6e287d96615cc8\",\n",
      "                \"document_name\": \"Full_Document_Markdown.md\",\n",
      "                \"private\": false,\n",
      "                \"collection_type\": \"user\",\n",
      "                \"parent_collection_hash_id\": \"6f053538b7f625ec98f451cac6089bb388a09973d32afbffc5a15b01e3c70305\",\n",
      "                \"document_id\": \"18c38772759cf3ee1f118c71ae82e36f50742d259952b0ea76411a6fe4f9fc77\",\n",
      "                \"website_url\": null,\n",
      "                \"text\": \"Character pictures or gifs, along with descriptions and storylines (good\\nguys, villains, etc.) \\u25cf Character information\\n\\nWe can use our character bio doc for the 10 main characters.\\n\\nWe\\\\'ll need to work on getting some bios created for some of the main\\nNPC characters.\\n\\nImportant note: We have a lot of content describing the gameplay, art,\\ncharacters, and writing. But we\\\\'re missing music, which is a major\\ncomponent of this game. Here are a couple ways we plan on using music to\\nhelp our world come to life:\\n\\nAdaptive Music\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"fa0148081672f49dd0dd1f24b508b3d6805f08b327b219bcee5950877c337984\",\n",
      "                \"document_integrity\": \"381b284211c603478c34b00443b21345b2c0971f77cdaaca3f6e287d96615cc8\",\n",
      "                \"document_name\": \"Full_Document_Markdown.md\",\n",
      "                \"private\": false,\n",
      "                \"collection_type\": \"user\",\n",
      "                \"parent_collection_hash_id\": \"6f053538b7f625ec98f451cac6089bb388a09973d32afbffc5a15b01e3c70305\",\n",
      "                \"document_id\": \"18c38772759cf3ee1f118c71ae82e36f50742d259952b0ea76411a6fe4f9fc77\",\n",
      "                \"website_url\": null,\n",
      "                \"text\": \"Brilliant engineer despite dressing from a dumpster. Their disorganized\\nbrain has a tendency to frankenstein parts together into some strange\\nbut functional contraption, whether or not that contraption is useful at\\nthat very moment. Joined the Top Dog competition because part of it\\ntakes place in the space station: they're looking to score some\\ndecommissioned tech.\\n\\nHas a mild fascination with conspiracy theories, though they don't\\nreally believe most of them.\\n\\nName: Finn\\n\\nBio:\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"c528423c08706d810d2c20232e8310cb176ac2044721bb59547ca64ac2c586dc\",\n",
      "                \"document_integrity\": \"381b284211c603478c34b00443b21345b2c0971f77cdaaca3f6e287d96615cc8\",\n",
      "                \"document_name\": \"Full_Document_Markdown.md\",\n",
      "                \"private\": false,\n",
      "                \"collection_type\": \"user\",\n",
      "                \"parent_collection_hash_id\": \"6f053538b7f625ec98f451cac6089bb388a09973d32afbffc5a15b01e3c70305\",\n",
      "                \"document_id\": \"18c38772759cf3ee1f118c71ae82e36f50742d259952b0ea76411a6fe4f9fc77\",\n",
      "                \"website_url\": null,\n",
      "                \"text\": \"10 main characters and their bios\\n\\nOther characters\\n\\nMain antagonist\\n\\nHost (is the host the main antagonist?)\\n\\nK.K. Slider Character (sings music in a mumble voice)\\n\\nTraveling Merchant\\n\\n{Boards Banner}\\n\\nWe should show off a few screenshots and maybe a clip or two of our\\nboard prototypes, along with a paragraph explaining the setting and the\\nchallenges the board will offer.\\n\\n{Game Modes Banner}\\n\\nEvery Mario Party game has alternate game modes, or at least different\\nways of playing minigames. We should describe what our game modes are.\\n\\nClassic\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "input.update({\n",
    "    # \"query\": \"What is representation learning?\",\n",
    "    \"query\": \"Who is Catherine?\",\n",
    "    \"collection_hash_ids\": [COLLECTION_ARGS[\"hash_id\"]],\n",
    "    # \"use_rerank\": True,\n",
    "    # \"minimum_relevance\": 0,\n",
    "})\n",
    "\n",
    "print(\"Collection Hash:\", COLLECTION_ARGS[\"hash_id\"])\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/query_vector_db\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "if \"trace\" in result:\n",
    "    print(result[\"trace\"])\n",
    "assert not (\"success\" in result and result[\"success\"] == False), json.dumps(result[\"trace\"], indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `craft_document_access_token`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true,\n",
      "    \"result\": {\n",
      "        \"file_name\": \"HNRS3035_08_22_2023_MLIntro.pdf\",\n",
      "        \"access_encrypted\": \"0417cddb6717935ab4825aa44ffe3e083cb70f51fc168e2f11fb9c8036fbb7442d5a5f586f0d96ee74a90631d6ad9f2d6db7d9a08825cda29cb9344dbc750cf941e1c0f2e871a9937cd9da64abbe01bf96dab6851999b9e58da7f6e6c48108193b527158b9545a2ac8d453ae3fcc2ecf1214bd06edf0e28e076d3d330a59626cd8c2a82617eba7aa8221612fb78dd891b1af447a5305b869bb3c5fc82222844515494e59099ad7b7c3f0ed9062b2bad49a732d6c523c35c4769768f2d341f7565917ecc37c078657727d30f8d5d955c76d1c0b983a77aabfcf92481ecb4b2afa7d94364f357c2be20b00f914120bc3e869d58febffe06bf157fee2900b3f7bc908cd3144efa85476149d39c4f8c688b15513c6463de01551537517cfc887ec25c288a47613c3df46456bd67ae6b3f67c2c563c70127ef90a2e165392c2492f955e799e447c5a31f3ccfb72d7a097b1af1dc4c245a303e44f3e9395f0fe7bed7ee36e176a9daeb490583d956a16af39745e72104bc14698c6441787e7f502148e1c5f42dfa9a2bffa612e2e8a89eecbdbc3fe642b08810292c07bd9870c876f5e82b0f22f237be0ddf9cc7704e0749d7508fdab2a097e9282c30a12c070abc9102e4664604ca384911cae7475fe6edea2f22483da8690211fe0fc2f76aeb76bfc8228f8b390fd951c062bd5adefa3f931bbe172b3c63b4d39afcf4bf58f0ccf012b4574d8c565cfd8c333767ec14a9979b1fb50bd7f390cffe2ea9c2a9aba9f9be21586269b\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "input.update(DOC_ARGS_2)\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/craft_document_access_token\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]\n",
    "\n",
    "DOCUMENT_AUTH_ACCESS = {\"document_auth_access\": result[\"result\"][\"access_encrypted\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `fetch_document`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8000/fetch_document?parameters=%7B%22document_auth_access%22%3A+%220417cddb6717935ab4825aa44ffe3e083cb70f51fc168e2f11fb9c8036fbb7442d5a5f586f0d96ee74a90631d6ad9f2d6db7d9a08825cda29cb9344dbc750cf941e1c0f2e871a9937cd9da64abbe01bf96dab6851999b9e58da7f6e6c48108193b527158b9545a2ac8d453ae3fcc2ecf1214bd06edf0e28e076d3d330a59626cd8c2a82617eba7aa8221612fb78dd891b1af447a5305b869bb3c5fc82222844515494e59099ad7b7c3f0ed9062b2bad49a732d6c523c35c4769768f2d341f7565917ecc37c078657727d30f8d5d955c76d1c0b983a77aabfcf92481ecb4b2afa7d94364f357c2be20b00f914120bc3e869d58febffe06bf157fee2900b3f7bc908cd3144efa85476149d39c4f8c688b15513c6463de01551537517cfc887ec25c288a47613c3df46456bd67ae6b3f67c2c563c70127ef90a2e165392c2492f955e799e447c5a31f3ccfb72d7a097b1af1dc4c245a303e44f3e9395f0fe7bed7ee36e176a9daeb490583d956a16af39745e72104bc14698c6441787e7f502148e1c5f42dfa9a2bffa612e2e8a89eecbdbc3fe642b08810292c07bd9870c876f5e82b0f22f237be0ddf9cc7704e0749d7508fdab2a097e9282c30a12c070abc9102e4664604ca384911cae7475fe6edea2f22483da8690211fe0fc2f76aeb76bfc8228f8b390fd951c062bd5adefa3f931bbe172b3c63b4d39afcf4bf58f0ccf012b4574d8c565cfd8c333767ec14a9979b1fb50bd7f390cffe2ea9c2a9aba9f9be21586269b%22%7D\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from urllib.parse import urlencode\n",
    "from copy import deepcopy\n",
    "\n",
    "# input = deepcopy(USER_ARGS_1)\n",
    "input = deepcopy(DOCUMENT_AUTH_ACCESS)\n",
    "\n",
    "encoded_params = urlencode({\"parameters\": json.dumps(input)})\n",
    "\n",
    "print(\"http://localhost:8000/fetch_document?\"+encoded_params)\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/fetch_document?\"+encoded_params)\n",
    "response.raise_for_status()\n",
    "\n",
    "# result = response.json()\n",
    "\n",
    "# print(json.dumps(result, indent=4))\n",
    "# assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `create_organization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true,\n",
      "    \"result\": {\n",
      "        \"organization_id\": 18,\n",
      "        \"organization_dict\": {\n",
      "            \"openai_organization_id_encrypted\": null,\n",
      "            \"hash_id\": \"9b9a795364034b29228e71cbe50dc711139bff08618b7e1222652e563405ffd1\",\n",
      "            \"name\": \"test_org\",\n",
      "            \"public_key\": \"3342df49390a3216dd984426c3bcc14e74ca53bf912739fad15c8af8a2a2009b5c8256a3cc8b035b9988453940eea4372f7eb5f2883b3281542437a76bb77f4a\",\n",
      "            \"creation_timestamp\": 1708396336.4899044,\n",
      "            \"id\": 18,\n",
      "            \"serp_api_key_encrypted\": null\n",
      "        },\n",
      "        \"membership_dict\": {}\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "input.update({\n",
    "    \"organization_name\": \"test_org\",\n",
    "})\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/create_organization\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]\n",
    "\n",
    "ORG_ARGS = result[\"result\"][\"organization_dict\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `set_organization_openai_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "input.update({\n",
    "    \"openai_organization_id\": \"org-1111111111111111111111111\"\n",
    "})\n",
    "input.update({\n",
    "    \"organization_hash_id\": ORG_ARGS[\"hash_id\"]\n",
    "})\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/set_organization_openai_id\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `invite_user_to_organization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "input.update({\n",
    "    \"username_to_invite\": USERNAME_2,\n",
    "    \"organization_id\": ORG_ARGS[\"id\"],\n",
    "    \"member_class\": \"member\"\n",
    "})\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/invite_user_to_organization\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `fetch_memberships`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true,\n",
      "    \"result\": {\n",
      "        \"memberships\": [\n",
      "            {\n",
      "                \"organization_id\": 18,\n",
      "                \"organization_name\": \"test_org\",\n",
      "                \"role\": \"member\",\n",
      "                \"invite_still_open\": true,\n",
      "                \"sender\": \"eb582139-4b27-42a4-8eb9-93638dac\"\n",
      "            }\n",
      "        ],\n",
      "        \"admin\": false\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_2)\n",
    "input.update({\n",
    "    \"return_subset\" : \"all\"\n",
    "})\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/fetch_memberships\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]\n",
    "INVITATION = result[\"result\"][\"memberships\"][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `resolve_organization_invitation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_2)\n",
    "input.update({\n",
    "    \"organization_id\": INVITATION[\"organization_id\"],\n",
    "    \"accept\": True\n",
    "})\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/resolve_organization_invitation\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `fetch_memberships_of_organization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true,\n",
      "    \"result\": {\n",
      "        \"memberships\": [\n",
      "            {\n",
      "                \"organization_id\": 18,\n",
      "                \"organization_name\": \"test_org\",\n",
      "                \"role\": \"owner\",\n",
      "                \"username\": \"eb582139-4b27-42a4-8eb9-93638dac\",\n",
      "                \"invite_still_open\": false\n",
      "            },\n",
      "            {\n",
      "                \"organization_id\": 18,\n",
      "                \"organization_name\": \"test_org\",\n",
      "                \"role\": \"member\",\n",
      "                \"username\": \"547a2c8f-6e44-4023-a8b7-f698824a\",\n",
      "                \"invite_still_open\": false\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "input.update({\n",
    "    \"organization_id\": INVITATION[\"organization_id\"],\n",
    "})\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/fetch_memberships_of_organization\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `set_user_serp_key`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "input.update({\n",
    "    \"serp_key\" : \"test_serp_key\"\n",
    "})\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/set_user_serp_key\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `set_organization_serp_key`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "input.update({\n",
    "    \"serp_key\" : \"test_serp_key\",\n",
    "    \"organization_hash_id\": ORG_ARGS[\"hash_id\"]\n",
    "})\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/set_organization_serp_key\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `get_serp_key`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": true,\n",
      "    \"result\": {\n",
      "        \"serp_key\": \"test_serp_key\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/get_serp_key\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❌ `search_google`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"success\": false,\n",
      "    \"note\": \"search_google() missing 1 required positional argument: 'query'\",\n",
      "    \"trace\": \"Traceback (most recent call last):\\n  File \\\"/home/kyle_m/QueryLake_Development/QueryLakeBackend/./server.py\\\", line 420, in api_general_call\\n    args_get = function_actual(**true_args)\\nTypeError: search_google() missing 1 required positional argument: 'query'\\n\"\n",
      "}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "search_google() missing 1 required positional argument: 'query'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[320], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m result \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(result, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m), result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnote\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: search_google() missing 1 required positional argument: 'query'"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/search_google\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❌ `perform_search_query`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/perform_search_query\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `api/llm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, time\n",
    "from copy import deepcopy\n",
    "\n",
    "prompt = \"What is the Riemann-Roch theorem?\"\n",
    "\n",
    "system_instruction = \"You are a general answering assistant that can comply with any request.\\n\\nYou always answer with markdown formatting. You will be penalized if you do not answer with markdown when it would be possible.\\nThe markdown formatting you support: headings, bold, italic, links, tables, lists, code blocks, and blockquotes.\\nYou do not support images and never include images. You will be penalized if you render images.\\n\\nAlways use markdown code blocks for code, and always use MathJax with LaTeX formatting for pure math expressions.\\nFor example code would look like:\\n```python\\n*Python code here\\n```\\nAnd math expressions would look like:\\n$$P(y|x) = \\\\frac{{P(x|y) \\\\cdot P(y)}}{{P(x)}}$$\"\n",
    "prompt_formatted = f\"<s>[INST] <<SYS>>\\n{system_instruction}\\n<</SYS>>\\n{prompt} [/INST] \"\n",
    "\n",
    "sample_input = {\n",
    "    \"model_choice\": \"mistral-7b-instruct-v0.1\",\n",
    "    \"prompt\": prompt_formatted, \n",
    "    # \"stream\": True, \n",
    "    \"stream_response_normal\": True,\n",
    "    \"max_tokens\": 1000, \n",
    "    \"temperature\": 0.5, \n",
    "    \"top_p\": 0.9, \n",
    "    \"repetition_penalty\": 1.15,\n",
    "    \"stop\": [\"</s>\"],\n",
    "}\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "input.update({\n",
    "    \"model_parameters\": sample_input\n",
    "})\n",
    "\n",
    "\n",
    "global_start_time = time.time()\n",
    "\n",
    "response = requests.get(f\"http://localhost:8000/api/llm\", json=input, stream=True)\n",
    "response.raise_for_status()\n",
    "response_completed = \"\"\n",
    "start_time, token_count = time.time(), 0\n",
    "for chunk_raw in response.iter_content(chunk_size=None, decode_unicode=True):\n",
    "    # print(chunk_raw)\n",
    "    if token_count == 0:\n",
    "        start_time = time.time()\n",
    "    chunk_decoded = chunk_raw.decode(\"utf-8\")\n",
    "    # print(chunk_decoded)\n",
    "    chunk = json.loads(chunk_decoded)\n",
    "    response_completed += chunk[\"text\"]\n",
    "    print(chunk[\"text\"], end=\"\")\n",
    "    \n",
    "    token_count += 1\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "print(\"\\n\\n\\nFINISHED RESPONSE WITH %5d tokens, %7.2f t/s, (%7.4f - %7.4f):\" % (token_count, (token_count-1) / time_taken, start_time - global_start_time, end_time - global_start_time), [prompt, response_completed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `api/rerank`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from threading import Thread\n",
    "import time, json\n",
    "from copy import deepcopy\n",
    "\n",
    "prompts_rerank = [\n",
    "    [\n",
    "        [\"What is the square root of 169?\", \"The square root of 169 is 13.\"],\n",
    "        [\"What is the derivative of sin(cos(x))?\", \"What is the derivative of sin(cos(x))? Well, it's a bit complicated. The derivative of sin(cos(x)) is cos(cos(x)) * -sin(x).\"],\n",
    "    ], [\n",
    "        [\"What is the square root of 169?\", \"cupcake\"],\n",
    "        [\"What is the derivative of sin(cos(x))?\", \"math\"],\n",
    "        [\"What is the square root of 169?\", \"math\"],\n",
    "        [\"What is the derivative of sin(cos(x))?\", \"math\"],\n",
    "    ]\n",
    "]\n",
    "\n",
    "def get_response(prompt_input):\n",
    "    \n",
    "    input = {\"auth\": {\"api_key\": API_KEY_1}}\n",
    "    input.update({\n",
    "        \"inputs\": prompt_input\n",
    "    })\n",
    "    \n",
    "    time_start = time.time()\n",
    "    response = requests.get(f\"http://localhost:8000/api/rerank\", json=input)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    response_value = response.json()\n",
    "    response_value = response_value[\"result\"]\n",
    "    time_end = time.time()\n",
    "    time_taken = time_end - time_start\n",
    "    \n",
    "    print(\"FINISHED RESPONSE in %5.2fs:\" % (time_taken), [prompt_input, response_value])\n",
    "\n",
    "print(\"RUNNING RERANK\")\n",
    "for p in prompts_rerank:\n",
    "    # time.sleep(0.5)\n",
    "    Thread(target=get_response, args=(p,)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `api/embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from threading import Thread\n",
    "import time, json\n",
    "from copy import deepcopy\n",
    "\n",
    "prompts_embeddings = [\n",
    "    [\n",
    "        \"What is the square root of 169?\",\n",
    "        \"What is the derivative of sin(cos(x))?\",\n",
    "    ], [\n",
    "        \"What is the square root of 169?\",\n",
    "        \"What is the derivative of sin(cos(x))?\",\n",
    "        \"What is the square root of 169?\",\n",
    "        \"What is the derivative of sin(cos(x))?\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "def get_response_embeddings(prompt_input):\n",
    "    time_start = time.time()\n",
    "    input = deepcopy(USER_ARGS_1)\n",
    "    input.update({\n",
    "        \"inputs\": prompt_input\n",
    "    })\n",
    "    \n",
    "    response = requests.get(f\"http://localhost:8000/api/embedding\", json=input)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    response_value = response.json()\n",
    "    response_value = response_value[\"result\"]\n",
    "    time_end = time.time()\n",
    "    time_taken = time_end - time_start\n",
    "    print(response_value)\n",
    "    \n",
    "    print(\"FINISHED RESPONSE in %5.2fs:\" % (time_taken), [prompt_input, len(response_value), len(response_value[0]), response_value])\n",
    "\n",
    "for p in prompts_embeddings:\n",
    "    # time.sleep(0.5)\n",
    "    Thread(target=get_response_embeddings, args=(p,)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toolchains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `get_available_toolchains`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/get_available_toolchains\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ `fetch_toolchain_sessions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from copy import deepcopy\n",
    "\n",
    "input = deepcopy(USER_ARGS_1)\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/api/fetch_toolchain_sessions\", json=input)\n",
    "response.raise_for_status()\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n",
    "assert not (\"success\" in result and result[\"success\"] == False), result[\"note\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ Toolchain Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from websockets.sync.client import connect, Connection\n",
    "from copy import deepcopy\n",
    "import time\n",
    "from typing import List, Union, Any\n",
    "from toolchain_efficient_receiver import wait_for_command_finish\n",
    "\n",
    "model_params_static = {\n",
    "    \"model_choice\": \"mistral-7b-instruct-v0.1\",\n",
    "    \"max_tokens\": 1000, \n",
    "    \"temperature\": 0.1, \n",
    "    \"top_p\": 0.1, \n",
    "    \"repetition_penalty\": 1.15,\n",
    "    \"stop\": [\"<|im_end|>\"],\n",
    "    \"include_stop_str_in_output\": True\n",
    "}\n",
    "\n",
    "toolchain_id = \"test_chat_session_normal\"\n",
    "\n",
    "# Append, update, delete. In that order.\n",
    "WS_STATE_GLOBAL = {}\n",
    "\n",
    "with connect(\"ws://localhost:8000/toolchain\") as websocket:\n",
    "    \n",
    "    input = deepcopy(USER_ARGS_1)\n",
    "    input.update({\n",
    "        \"command\" : \"toolchain/create\",\n",
    "        \"arguments\": {\n",
    "            \"toolchain_id\": toolchain_id\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    websocket.send(json.dumps(input))\n",
    "    print(\"\\n\\n\")\n",
    "   \n",
    "    result_1, WS_STATE_GLOBAL = await wait_for_command_finish(websocket, WS_STATE_GLOBAL)\n",
    "    print(\"RESULT 1\")\n",
    "    print(json.dumps(result_1, indent=4))\n",
    "    \n",
    "    \n",
    "    session_id = result_1[\"toolchain_session_id\"]\n",
    "    \n",
    "    input = deepcopy(USER_ARGS_1)\n",
    "    input.update({\n",
    "        \"command\" : \"toolchain/event\",\n",
    "        \"arguments\": {\n",
    "            \"session_id\": session_id,\n",
    "            \"event_node_id\": \"user_question_event\",\n",
    "            \"event_parameters\": {\n",
    "                \n",
    "                \"model_parameters\": model_params_static,\n",
    "                \"question\": \"What is the Riemann-Roch theorem?\"\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    websocket.send(json.dumps(input))\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    result_2, WS_STATE_GLOBAL = await wait_for_command_finish(websocket, WS_STATE_GLOBAL)\n",
    "    print(\"RESULT 2\")\n",
    "    print(json.dumps(result_2, indent=4))\n",
    "    \n",
    "    \n",
    "    input = deepcopy(USER_ARGS_1)\n",
    "    input.update({\n",
    "        \"command\" : \"toolchain/event\",\n",
    "        \"arguments\": {\n",
    "            \"session_id\": session_id,\n",
    "            \"event_node_id\": \"user_question_event\",\n",
    "            \"event_parameters\": {\n",
    "                \"model_parameters\": model_params_static,\n",
    "                \"question\": \"Who are the two people the Riemann-Roch Theorem is named after?\" \n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    websocket.send(json.dumps(input))\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    result_3, WS_STATE_GLOBAL = await wait_for_command_finish(websocket, WS_STATE_GLOBAL)\n",
    "    print(\"RESULT 3\")\n",
    "    print(json.dumps(result_3, indent=4))\n",
    "    \n",
    "    \n",
    "    input = deepcopy(USER_ARGS_1)\n",
    "    input.update({\n",
    "        \"command\" : \"toolchain/event\",\n",
    "        \"arguments\": {\n",
    "            \"session_id\": session_id,\n",
    "            \"event_node_id\": \"user_question_event\",\n",
    "            \"event_parameters\": {\n",
    "                \"model_parameters\": model_params_static,\n",
    "                \"question\": \"You're wrong. It was named after Gustav Roch.\"\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    websocket.send(json.dumps(input))\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    result_4, WS_STATE_GLOBAL = await wait_for_command_finish(websocket, WS_STATE_GLOBAL)\n",
    "    print(\"RESULT 4\")\n",
    "    print(json.dumps(result_4, indent=4))\n",
    "    \n",
    "    \n",
    "    print(\"\\n\\nFINAL TOOLCHAIN STATE\")\n",
    "    print(json.dumps(WS_STATE_GLOBAL, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ Toolchain Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from websockets.sync.client import connect, Connection\n",
    "from copy import deepcopy\n",
    "import time\n",
    "from typing import List, Union, Any\n",
    "from toolchain_efficient_receiver import wait_for_command_finish\n",
    "\n",
    "model_params_static = {\n",
    "    \"model_choice\": \"mistral-7b-instruct-v0.1\",\n",
    "    # \"stream\": True, \n",
    "    # \"stream_response_normal\": True,\n",
    "    \"max_tokens\": 1000, \n",
    "    \"temperature\": 0.1, \n",
    "    \"top_p\": 0.1, \n",
    "    \"repetition_penalty\": 1.15,\n",
    "    \"stop\": [\"<|im_end|>\"],\n",
    "    \"include_stop_str_in_output\": True\n",
    "}\n",
    "\n",
    "toolchain_id = \"test_chat_session_normal_streaming\"\n",
    "\n",
    "# Append, update, delete. In that order.\n",
    "WS_STATE_GLOBAL = {}\n",
    "\n",
    "with connect(\"ws://localhost:8000/toolchain\") as websocket:\n",
    "    \n",
    "    input = deepcopy(USER_ARGS_1)\n",
    "    input.update({\n",
    "        \"command\" : \"toolchain/create\",\n",
    "        \"arguments\": {\n",
    "            \"toolchain_id\": toolchain_id\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    websocket.send(json.dumps(input))\n",
    "    print(\"\\n\\n\")\n",
    "   \n",
    "    result_1, WS_STATE_GLOBAL = await wait_for_command_finish(websocket, WS_STATE_GLOBAL)\n",
    "    \n",
    "    session_id = result_1[\"toolchain_session_id\"]\n",
    "    \n",
    "    input = deepcopy(USER_ARGS_1)\n",
    "    input.update({\n",
    "        \"command\" : \"toolchain/event\",\n",
    "        \"arguments\": {\n",
    "            \"session_id\": session_id,\n",
    "            \"event_node_id\": \"user_question_event\",\n",
    "            \"event_parameters\": {\n",
    "                \n",
    "                \"model_parameters\": model_params_static,\n",
    "                \"question\": \"What is the Riemann-Roch theorem?\"\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    websocket.send(json.dumps(input))\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    result_2, WS_STATE_GLOBAL = await wait_for_command_finish(websocket, WS_STATE_GLOBAL)\n",
    "    \n",
    "    input = deepcopy(USER_ARGS_1)\n",
    "    input.update({\n",
    "        \"command\" : \"toolchain/event\",\n",
    "        \"arguments\": {\n",
    "            \"session_id\": session_id,\n",
    "            \"event_node_id\": \"user_question_event\",\n",
    "            \"event_parameters\": {\n",
    "                \"model_parameters\": model_params_static,\n",
    "                \"question\": \"Who are the two people the Riemann-Roch Theorem is named after?\" \n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    websocket.send(json.dumps(input))\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    result_3, WS_STATE_GLOBAL = await wait_for_command_finish(websocket, WS_STATE_GLOBAL)\n",
    "    \n",
    "    input = deepcopy(USER_ARGS_1)\n",
    "    input.update({\n",
    "        \"command\" : \"toolchain/event\",\n",
    "        \"arguments\": {\n",
    "            \"session_id\": session_id,\n",
    "            \"event_node_id\": \"user_question_event\",\n",
    "            \"event_parameters\": {\n",
    "                \"model_parameters\": model_params_static,\n",
    "                \"question\": \"You're wrong. It was named after Gustav Roch.\"\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    websocket.send(json.dumps(input))\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    result_4, WS_STATE_GLOBAL = await wait_for_command_finish(websocket, WS_STATE_GLOBAL)\n",
    "    \n",
    "    print(\"\\n\\nFINAL TOOLCHAIN STATE\")\n",
    "    print(json.dumps(WS_STATE_GLOBAL, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toolchain Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from websockets.sync.client import connect, Connection\n",
    "from copy import deepcopy\n",
    "import time\n",
    "from typing import List, Union, Any\n",
    "from toolchain_efficient_receiver import wait_for_command_finish\n",
    "\n",
    "model_params_static = {\n",
    "    \"model_choice\": \"mistral-7b-instruct-v0.1\",\n",
    "    \"max_tokens\": 1000, \n",
    "    \"temperature\": 0.1, \n",
    "    \"top_p\": 0.1, \n",
    "    \"repetition_penalty\": 1.15,\n",
    "    \"stop\": [\"<|im_end|>\"],\n",
    "    \"include_stop_str_in_output\": True\n",
    "}\n",
    "\n",
    "toolchain_id = \"iterable_test\"\n",
    "\n",
    "# Append, update, delete. In that order.\n",
    "WS_STATE_GLOBAL_2 = {}\n",
    "\n",
    "with connect(\"ws://localhost:8000/toolchain\") as websocket:\n",
    "    \n",
    "    input = deepcopy(USER_ARGS_1)\n",
    "    input.update({\n",
    "        \"command\" : \"toolchain/create\",\n",
    "        \"arguments\": {\n",
    "            \"toolchain_id\": toolchain_id\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    websocket.send(json.dumps(input))\n",
    "    print(\"\\n\\n\")\n",
    "   \n",
    "    result_1, WS_STATE_GLOBAL_2 = await wait_for_command_finish(websocket, WS_STATE_GLOBAL_2)\n",
    "    print(\"RESULT 1\")\n",
    "    print(json.dumps(result_1, indent=4))\n",
    "    \n",
    "    \n",
    "    session_id = result_1[\"toolchain_session_id\"]\n",
    "    \n",
    "    input = deepcopy(USER_ARGS_1)\n",
    "    input.update({\n",
    "        \"command\" : \"toolchain/event\",\n",
    "        \"arguments\": {\n",
    "            \"session_id\": session_id,\n",
    "            \"event_node_id\": \"user_question_event\",\n",
    "            \"event_parameters\": {\n",
    "                \"model_parameters\": model_params_static\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    websocket.send(json.dumps(input))\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    result_2, WS_STATE_GLOBAL_2 = await wait_for_command_finish(websocket, WS_STATE_GLOBAL_2)\n",
    "    print(\"RESULT 2\")\n",
    "    print(json.dumps(result_2, indent=4))\n",
    "    print(\"END RESULT 2\")\n",
    "    \n",
    "    print(\"\\n\\nFINAL TOOLCHAIN STATE\")\n",
    "    print(json.dumps(WS_STATE_GLOBAL_2, indent=4))\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QL_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
