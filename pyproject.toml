[build-system]
requires = ["hatchling>=1.25.0"]
build-backend = "hatchling.build"

[project]
name = "querylake-backend"
version = "0.0.0"
description = "QueryLake Backend (FastAPI + Ray Serve)"
readme = "README.md"
requires-python = ">=3.10,<3.14"
license = { text = "Apache-2.0" }
authors = [
  { name = "QueryLake Team" }
]
keywords = ["querylake", "rag", "retrieval", "ray-serve", "fastapi", "llm"]
classifiers = [
  "Development Status :: 4 - Beta",
  "Intended Audience :: Developers",
  "Intended Audience :: Science/Research",
  "Programming Language :: Python :: 3",
  "Programming Language :: Python :: 3.10",
  "Programming Language :: Python :: 3.11",
  "Programming Language :: Python :: 3.12",
  "License :: OSI Approved :: Apache Software License",
]

# NOTE:
# - Base dependencies target an "API-only" environment (no local inference engines).
# - Heavier stacks (HF/torch, OCR, vLLM) live in optional extras.
dependencies = [
  # Core API
  "fastapi",
  "uvicorn",
  "sse-starlette",
  "python-multipart>=0.0.22",

  # HTTP clients / streaming
  "httpx",
  "httpx-sse",

  # Auth / crypto
  "PyJWT>=2.10.1",
  "passlib",
  "fastapi-login",
  "cryptography",
  "pycryptodome",
  "eciespy",
  "eth-keys",

  # DB / storage
  "sqlalchemy==2.0.35",
  "sqlmodel==0.0.22",
  "psycopg2-binary==2.9.9",
  "pgvector",
  "redis",

  # Utilities
  "tiktoken",
  "openai",
  "jsonpatch",
  "jsonpath-ng",

  # Document + web tooling (non-ML)
  "pypdf",
  "pypdfium2",
  "py7zr",
  "markdownify",
  "readability-lxml",
  "lxml-html-clean",
  "selenium",

  # Web search provider
  "google-search-results",

  # Ray Serve (core runtime) â€” keep pinned to match the cluster Ray version.
  "ray[serve]==2.53.0",

  # GPU inspection (VRAM-aware scheduling, diagnostics)
  "nvidia-ml-py",
]

[project.optional-dependencies]
# CLI / setup helpers (interactive config + HF downloads).
cli = [
  "colorama",
  "huggingface_hub[cli]",
]

# Local HF / torch inference (embeddings, rerank, etc).
inference-hf = [
  "torch",
  "transformers",
  "sentence-transformers",
  "FlagEmbedding",
  "huggingface_hub[cli]",
  "safetensors",
  "sentencepiece>=0.1.99",
  "lm-format-enforcer",
  "peft",
]

# OCR stack (Marker/Surya + optional OCRmyPDF).
ocr = [
  "torch",
  "pillow",
  "ocrmypdf",
  "marker-pdf==0.3.9",
  "surya-ocr==0.6.11",
]

# vLLM runtime is intentionally separate in production (run as upstream HTTP service),
# but is provided here as an optional extra for experiments.
vllm = [
  "vllm>=0.14.1",
]

dev = [
  "pytest",
  "pytest-asyncio",
]

[project.urls]
Homepage = "https://github.com/QueryLake/QueryLake"
Documentation = "https://github.com/QueryLake/QueryLakeBackend/tree/main/docs"
Source = "https://github.com/QueryLake/QueryLakeBackend"
Issues = "https://github.com/QueryLake/QueryLakeBackend/issues"

[project.scripts]
querylake-backend = "start_querylake:main"

[tool.hatch.build.targets.wheel]
packages = ["QueryLake"]
