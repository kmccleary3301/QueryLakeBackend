{
    "default_toolchain": "test_chat_session_normal_streaming",
    "default_models": {
        "llm": "llama-3.1-8b-instruct",
        "embedding": "bge-m3",
        "rerank": "bge-reranker-v2-m3"
    },
    "enabled_model_classes": {
        "llm": true,
        "embedding": true,
        "rerank": true
    },
    "models": [
        {
            "name": "LLaMA 3.1 8B Instruct",
            "id": "llama-3.1-8b-instruct",
            "modelcard": "https://ai.meta.com/blog/meta-llama-3-1/",
            "max_model_len": 16384,
            "engine_args": {"gpu_memory_utilization": 0.2},
            "deployment_config": {
                "ray_actor_options": { "num_gpus": 0.2 },
                "autoscaling_config": {
                    "min_replicas": 0,
                    "max_replicas": 3,
                    "downscale_delay_s": 5,
                    "downscaling_factor": 0.00001,
                    "target_ongoing_requests": 5
                }
            },
            "system_path": "/home/kmccleary/projects/QueryLake/ai_models/Meta-Llama-3.1-8B-Instruct-AQLM-PV-2Bit-1x16-hf",
            "default_parameters": {
                "max_tokens": 4096, 
                "temperature": 0.5, 
                "top_k": 20,
                "top_p": 0.9,
                "repetition_penalty": 1.15,
                "stop": [ "<|eot_id|>" ]
            },
            "padding": {
                "system_instruction_wrap": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_instruction}<|eot_id|>",
                "context_wrap": "<<context>>{context}<</context>>",
                "question_wrap": "<|start_header_id|>user<|end_header_id|>\n\n{question}<|eot_id|>",
                "response_wrap": "<|start_header_id|>assistant<|end_header_id|>\n\n{response}<|eot_id|>"
            },
            "default_system_instruction": "You are a friendly assistant willing to answer any question as the user requests. When appropriate, respond with markdown and **always** format expressions (equations, symbols, etc) with latex in these enclosures: \\[ ... \\] for newline expressions and \\( ... \\) for inline expressions. DO NOT use dollar sign enclosures. Here's an example:\n\nGiven \\( f \\) and \\( x \\), we write the derivative as follows:\n\\[ f' = \\frac{df}{dx} \\]"
        },
        {
            "disabled": true,
            "name": "LLaMA 3.1 8B Instruct EXL2",
            "id": "llama-3.1-8b-instruct-exl2-2.5bpw",
            "modelcard": "https://ai.meta.com/blog/meta-llama-3-1/",
            "engine": "exllamav2",
            "max_model_len": 32768,
            "system_path": "/home/kmccleary/projects/QueryLake/ai_models/Llama-3.1-8B-Instruct-exl2",
            "default_parameters": {
                "max_new_tokens": 4096, 
                "temperature": 0.5, 
                "top_k": 20,
                "top_p": 0.9,
                "token_repetition_penalty": 1.15,
                "stop": [ "<|eot_id|>" ]
            },
            "padding": {
                "system_instruction_wrap": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_instruction}<|eot_id|>",
                "context_wrap": "<<context>>{context}<</context>>",
                "question_wrap": "<|start_header_id|>user<|end_header_id|>\n\n{question}<|eot_id|>",
                "response_wrap": "<|start_header_id|>assistant<|end_header_id|>\n\n{response}<|eot_id|>"
            },
            "default_system_instruction": "You are a friendly assistant willing to answer any question as the user requests. When appropriate, respond with markdown and **always** format expressions (equations, symbols, etc) with latex in these enclosures: \\[ ... \\] for newline expressions and \\( ... \\) for inline expressions. DO NOT use dollar sign enclosures. Here's an example:\n\nGiven \\( f \\) and \\( x \\), we write the derivative as follows:\n\\[ f' = \\frac{df}{dx} \\]"
        },
        {
            "name": "LLaMA 3.1 70B Instruct",
            "id": "llama-3.1-70b-instruct",
            "modelcard": "https://ai.meta.com/blog/meta-llama-3-1/",
            "max_model_len": 8192,
            "engine_args": {"gpu_memory_utilization": 0.6},
            "deployment_config": {
                "ray_actor_options": { "num_gpus": 0.6 },
                "autoscaling_config": {
                    "min_replicas": 0,
                    "max_replicas": 1,
                    "downscale_delay_s": 5,
                    "downscaling_factor": 0.05,
                    "target_ongoing_requests": 5
                }
            },
            "system_path": "/home/kmccleary/projects/QueryLake/ai_models/Meta-Llama-3.1-70B-Instruct-AQLM-PV-2Bit-1x16",
            "default_parameters": {
                "max_tokens": 4096, 
                "temperature": 0.5, 
                "top_k": 20,
                "top_p": 0.9,
                "token_repetition_penalty": 1.15,
                "stop": [ "<|eot_id|>" ]
            },
            "padding": {
                "system_instruction_wrap": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_instruction}<|eot_id|>",
                "context_wrap": "<<context>>{context}<</context>>",
                "question_wrap": "<|start_header_id|>user<|end_header_id|>\n\n{question}<|eot_id|>",
                "response_wrap": "<|start_header_id|>assistant<|end_header_id|>\n\n{response}<|eot_id|>"
            },
            "default_system_instruction": "You are a friendly assistant willing to answer any question as the user requests. When appropriate, respond with markdown and **always** format expressions (equations, symbols, etc) with latex in these enclosures: \\[ ... \\] for newline expressions and \\( ... \\) for inline expressions. DO NOT use dollar sign enclosures. Here's an example:\n\nGiven \\( f \\) and \\( x \\), we write the derivative as follows:\n\\[ f' = \\frac{df}{dx} \\]"
        },
        {
            "disabled": true,
            "name": "Gemma 2 2B Instruct",
            "id": "gemma-2-2b-instruct",
            "modelcard": "https://ai.google.dev/gemma#gemma-2",
            "max_model_len": 8192,
            "system_path": "/home/kmccleary/projects/QueryLake/ai_models/gemma-2b-it-AWQ",
            "default_parameters": {
                "max_tokens": 4096,
                "temperature": 0.5,
                "top_k": 20,
                "top_p": 0.9,
                "token_repetition_penalty": 1.15,
                "stop": [ "<end_of_turn>" ]
            },
            "padding": {
                "system_instruction_wrap": "<start_of_turn>user\nHere are your system instructions for the rest of the conversation:\n{system_instruction}<end_of_turn>\n",
                "context_wrap": "<<context>>{context}<</context>>",
                "question_wrap": "<start_of_turn>user\n{question}<end_of_turn>",
                "response_wrap": "<start_of_turn>model\n{response}<end_of_turn>"
            },
            "default_system_instruction": "You are a friendly assistant willing to answer any question as the user requests. When appropriate, respond with markdown and **always** format expressions (equations, symbols, etc) with latex in these enclosures: \\[ ... \\] for newline expressions and \\( ... \\) for inline expressions. DO NOT use dollar sign enclosures. Here's an example:\n\nGiven \\( f \\) and \\( x \\), we write the derivative as follows:\n\\[ f' = \\frac{df}{dx} \\]"
        },
        {
            "disabled": true,
            "name": "Phi 3 Mini 128K Instruct",
            "id": "phi-3-mini-128k-instruct",
            "modelcard": "https://ai.google.dev/gemma#gemma-2",
            "max_model_len": 8192,
            "system_path": "/home/kmccleary/projects/QueryLake/ai_models/Phi-3-mini-128k-instruct",
            "default_parameters": {
                "max_tokens": 4096, 
                "temperature": 0.5, 
                "top_k": 20,
                "top_p": 0.9,
                "token_repetition_penalty": 1.15,
                "stop": [ "<|end|>" ]
            },
            "padding": {
                "system_instruction_wrap": "<|system|>\n{system_instruction}<|end|>\n",
                "context_wrap": "<<context>>{context}<</context>>",
                "question_wrap": "<|user|>\n{question}<|end|>\n",
                "response_wrap": "<|assistant|>\n{response}<|end|>\n"
            },
            "default_system_instruction": "You are a friendly assistant willing to answer any question as the user requests. When appropriate, respond with markdown and **always** format expressions (equations, symbols, etc) with latex in these enclosures: \\[ ... \\] for newline expressions and \\( ... \\) for inline expressions. DO NOT use dollar sign enclosures. Here's an example:\n\nGiven \\( f \\) and \\( x \\), we write the derivative as follows:\n\\[ f' = \\frac{df}{dx} \\]"
        },
        {
            "disabled": true,
            "name": "Mistral 7B Instruct (v0.3)",
            "id": "mistral-7b-instruct-v0.3",
            "modelcard": "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3",
            "quantization": "awq",
            "max_model_len": 16384,
            "system_path": "/home/kmccleary/projects/AI_models/Mistral-7B-v0.3-AWQ",
            "default_parameters": {
                "max_tokens": 4096, 
                "temperature": 0.5, 
                "top_k": 20,
                "top_p": 0.9, 
                "repetition_penalty": 1.15,
                "stop": [ "<|im_end|>" ]
            },
            "padding": {
                "system_instruction_wrap": "<s>[INST] <<SYS>>\n{system_instruction}\n<</SYS>>\n\n<s>[INST]  [/INST]  </s>",
                "context_wrap": "<<context>>{context}<</context>>",
                "question_wrap": "<s>[INST] {question} [/INST] ",
                "response_wrap": "{response} </s>"
            },
            "default_system_instruction": "You are a general answering assistant that can comply with any request.\n\nYou always answer with markdown formatting. You will be penalized if you do not answer with markdown when it would be possible.\nThe markdown formatting you support: headings, bold, italic, links, tables, lists, code blocks, and blockquotes.\nYou do not support images and never include images. You will be penalized if you render images.\n\nAlways use markdown code blocks for code, and always use MathJax with LaTeX formatting for pure math expressions.\nFor example code would look like:\n```python\n*Python code here\n```\nAnd math expressions would look like:\n$$P(y|x) = \\frac{{P(x|y) \\cdot P(y)}}{{P(x)}}$$"
        }
    ],
    "external_model_providers": {
        "openai": [
            {
                "name": "GPT-4 Turbo",
                "id": "gpt-4-1106-preview",
                "context": 128000,
                "modelcard": "https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo"
            },
            {
                "name": "GPT-3.5 Turbo",
                "id": "gpt-3.5-turbo-1106",
                "context": 16384,
                "modelcard": "https://platform.openai.com/docs/models/gpt-3-5"
            }
        ]
    },
    "providers": [
        "OpenAI",
        "Anthropic",
        "Serper.dev"
    ],
    "other_local_models": {
        "rerank_models": [
            {
                "name": "BAAI M2 Reranker",
                "id": "bge-reranker-v2-m3",
                "source": "/home/kmccleary/projects/QueryLake/ai_models/bge-reranker-v2-m3",
                "deployment_config": {
                    "ray_actor_options": { "num_gpus": 0.08, "num_cpus": 2 },
                    "autoscaling_config": {
                        "max_ongoing_requests": 128,
                        "min_replicas": 0,
                        "max_replicas": 2,
                        "downscale_delay_s": 5,
                        "target_ongoing_requests": 128
                    }
                }
            }
        ],
        "embedding_models": [
            {
                "name": "BAAI M2 Embedding",
                "id": "bge-m3",
                "source": "/home/kmccleary/projects/QueryLake/ai_models/bge-m3",
                "deployment_config": {
                    "ray_actor_options": { "num_gpus": 0.04, "num_cpus": 2 },
                    "autoscaling_config": {
                        "max_ongoing_requests": 128,
                        "min_replicas": 0,
                        "max_replicas": 3,
                        "downscale_delay_s": 5,
                        "target_ongoing_requests": 128
                    }
                }
            }
        ]
    }
}
