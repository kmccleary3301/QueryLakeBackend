{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/querylake_manager/miniconda3/envs/QL_1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-17 17:16:47,601\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from QueryLake.operation_classes.ray_vllm_class import VLLMDeploymentClass, Model\n",
    "\n",
    "config_args = {\n",
    "\t\"name\": \"Molmo 7B D 0924\",\n",
    "\t\"id\": \"molmo-7b-d\",\n",
    "\t\"source\": \"allenai/Molmo-7B-D-0924\",\n",
    "\t\"engine\": \"vllm\",\n",
    "\t\"modelcard\": \"https://huggingface.co/allenai/Molmo-7B-D-0924\",\n",
    "\t\"system_path\": \"/shared_folders/querylake_server/QueryLakeBackend/models/models--allenai--Molmo-7B-D-0924/snapshots/1721478b71306fb7dc671176d5c204dc7a4d27d7\",\n",
    "\t\"default_parameters\": {\n",
    "\t\t\"max_tokens\": 4096,\n",
    "\t\t\"temperature\": 0.5,\n",
    "\t\t\"top_k\": 20,\n",
    "\t\t\"top_p\": 0.9,\n",
    "\t\t\"repetition_penalty\": 1.15,\n",
    "\t\t\"stop\": [\n",
    "\t\t\t\"<|eot_id|>\"\n",
    "\t\t]\n",
    "\t},\n",
    "\t\"max_model_len\": 4096,\n",
    "\t\"padding\": {\n",
    "\t\t\"system_instruction_wrap\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_instruction}<|eot_id|>\",\n",
    "\t\t\"context_wrap\": \"<<context>>{context}<</context>>\",\n",
    "\t\t\"question_wrap\": \"<|start_header_id|>user<|end_header_id|>\\n\\n{question}<|eot_id|>\",\n",
    "\t\t\"response_wrap\": \"<|start_header_id|>assistant<|end_header_id|>\\n\\n{response}<|eot_id|>\"\n",
    "\t},\n",
    "\t\"default_system_instruction\": \"You are a friendly assistant willing to answer any question as the user requests. When appropriate, respond with markdown and **always** format expressions (equations, symbols, etc) with latex in these enclosures: \\\\[ ... \\\\] for newline expressions and \\\\( ... \\\\) for inline expressions. DO NOT use dollar sign enclosures. Here's an example:\\n\\nGiven \\\\( f \\\\) and \\\\( x \\\\), we write the derivative as follows:\\n\\\\[ f' = \\\\frac{df}{dx} \\\\]\",\n",
    "\t\"disabled\": False,\n",
    "\t\"engine_args\": {\n",
    "\t\t\"gpu_memory_utilization\": 0.4,\n",
    "\t\t\"trust_remote_code\": True,\n",
    "\t\t\"enforce_eager\": True\n",
    "\t},\n",
    "\t\"deployment_config\": {\n",
    "\t\t\"ray_actor_options\": {\n",
    "\t\t\t\"num_gpus\": 0.4\n",
    "\t\t},\n",
    "\t\t\"autoscaling_config\": {\n",
    "\t\t\t\"min_replicas\": 1,\n",
    "\t\t\t\"max_replicas\": 3,\n",
    "\t\t\t\"downscale_delay_s\": 5,\n",
    "\t\t\t\"downscaling_factor\": 1e-05,\n",
    "\t\t\t\"target_ongoing_requests\": 5\n",
    "\t\t}\n",
    "\t}\n",
    "}\n",
    "\n",
    "\n",
    "model_config = Model(**config_args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZING VLLM DEPLOYMENT\n",
      "INFO 10-17 17:16:48 config.py:1664] Downcasting torch.float32 to torch.float16.\n",
      "WARNING 10-17 17:16:51 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 10-17 17:16:51 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post2.dev3+geca2c5f7) with config: model='/shared_folders/querylake_server/QueryLakeBackend/models/models--allenai--Molmo-7B-D-0924/snapshots/1721478b71306fb7dc671176d5c204dc7a4d27d7', speculative_config=None, tokenizer='/shared_folders/querylake_server/QueryLakeBackend/models/models--allenai--Molmo-7B-D-0924/snapshots/1721478b71306fb7dc671176d5c204dc7a4d27d7', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/shared_folders/querylake_server/QueryLakeBackend/models/models--allenai--Molmo-7B-D-0924/snapshots/1721478b71306fb7dc671176d5c204dc7a4d27d7, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-17 17:16:52 model_runner.py:1056] Starting to load model /shared_folders/querylake_server/QueryLakeBackend/models/models--allenai--Molmo-7B-D-0924/snapshots/1721478b71306fb7dc671176d5c204dc7a4d27d7...\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-17 17:16:52 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  14% Completed | 1/7 [00:00<00:01,  4.18it/s]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 2/7 [00:00<00:02,  1.96it/s]\n",
      "Loading safetensors checkpoint shards:  43% Completed | 3/7 [00:01<00:02,  1.59it/s]\n",
      "Loading safetensors checkpoint shards:  57% Completed | 4/7 [00:02<00:02,  1.41it/s]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 5/7 [00:03<00:01,  1.55it/s]\n",
      "Loading safetensors checkpoint shards:  86% Completed | 6/7 [00:03<00:00,  1.53it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:04<00:00,  1.48it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:04<00:00,  1.57it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-17 17:16:57 model_runner.py:1067] Loading model weights took 14.9975 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/querylake_manager/miniconda3/envs/QL_1/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/querylake_manager/miniconda3/envs/QL_1/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-17 17:16:59 gpu_executor.py:122] # GPU blocks: 1801, # CPU blocks: 4681\n",
      "INFO 10-17 17:16:59 gpu_executor.py:126] Maximum concurrency for 4096 tokens per request: 7.04x\n",
      "DONE INITIALIZING VLLM DEPLOYMENT\n"
     ]
    }
   ],
   "source": [
    "deployment = VLLMDeploymentClass(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def safe_serialize(obj, **kwargs) -> str:\n",
    "\t\"\"\"\n",
    "\tSerialize an object, but if an element is not serializable, return a string representation of said element.\n",
    "\t\"\"\"\n",
    "\tserialize_attempts = [\n",
    "\t\tlambda d: d.__dict__,\n",
    "\t\tlambda d: d.dict(),\n",
    "\t\tlambda d: d.to_dict()\n",
    "\t]\n",
    "    \n",
    "\tdef default_callable(o):\n",
    "\t\tnonlocal serialize_attempts\n",
    "        \n",
    "\t\tfor attempt in serialize_attempts:\n",
    "\t\t\ttry:\n",
    "\t\t\t\treturn attempt(o)\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "        \n",
    "\t\treturn f\"<<non-serializable: {type(o).__qualname__}>>\"\n",
    "    \n",
    "\t# default = lambda o: f\"<<non-serializable: {type(o).__qualname__}>>\"\n",
    "\tdefault = lambda o: default_callable(o)\n",
    "\treturn json.dumps(obj, default=default, **kwargs)\n",
    "\n",
    "# print(response.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history: [\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": [\n",
      "            {\n",
      "                \"type\": \"text\",\n",
      "                \"text\": \"Describe this image to me:\"\n",
      "            },\n",
      "            {\n",
      "                \"type\": \"image_url\",\n",
      "                \"image_url\": {\n",
      "                    \"url\": \"https://preview.redd.it/vb1h8ahxlzud1.png?width=1165&format=png&auto=webp&s=b9e929f358f94150cf79f0ed4577d591f1e652eb\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "Multi modal input: True\n",
      "ENCODING CHAT HISTORY: [\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": [\n",
      "            {\n",
      "                \"type\": \"text\",\n",
      "                \"text\": \"Describe this image to me:\"\n",
      "            },\n",
      "            {\n",
      "                \"type\": \"image_url\",\n",
      "                \"image_url\": {\n",
      "                    \"url\": \"https://preview.redd.it/vb1h8ahxlzud1.png?width=1165&format=png&auto=webp&s=b9e929f358f94150cf79f0ed4577d591f1e652eb\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "Wrapping chat entries: [2]\n",
      "ENCODED CHAT HISTORY: \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a friendly assistant willing to answer any question as the user requests. When appropriate, respond with markdown and **always** format expressions (equations, symbols, etc) with latex in these enclosures: \\\\[ ... \\\\] for newline expressions and \\\\( ... \\\\) for inline expressions. DO NOT use dollar sign enclosures. Here's an example:\\n\\nGiven \\\\( f \\\\) and \\\\( x \\\\), we write the derivative as follows:\\n\\\\[ f' = \\\\frac{df}{dx} \\\\]<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nDescribe this image to me:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
      "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a friendly assistant willing to answer any question as the user requests. When appropriate, respond with markdown and **always** format expressions (equations, symbols, etc) with latex in these enclosures: \\\\[ ... \\\\] for newline expressions and \\\\( ... \\\\) for inline expressions. DO NOT use dollar sign enclosures. Here's an example:\\n\\nGiven \\\\( f \\\\) and \\\\( x \\\\), we write the derivative as follows:\\n\\\\[ f' = \\\\frac{df}{dx} \\\\]<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nDescribe this image to me:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
      "Prompt: {\n",
      "    \"prompt\": \"User: Describe this image to me: Assistant:\",\n",
      "    \"multi_modal_data\": {\n",
      "        \"image\": \"<<non-serializable: Image>>\"\n",
      "    }\n",
      "}\n",
      " This image displays a chat interface, likely from an AI chatbot or a002 chat program. The screen is divided into two main sections: a conversation between the user and an AI, and a user input area at the bottom.\n",
      "\n",
      "The chat history includes a prompts like \"Are you asking for you, or for me?\" and \"My processors stutter, attempting to process your suggestive query.\" The AI's responses are noticeably stuttered and lack complexity, which could indicate it's not functioning correctly or is experiencing some kind of error.\n",
      "\n",
      "At the top of the interface, there are menu options for \"New Session,\" \"Scenarios,\" \"Save/Load,\" and \"Settings.\" The current version is listed as \"1.75.2,\" suggesting this might be an experimental or beta version of the software.\n",
      "\n",
      "The input area at the bottom prompts the user to \"Enter text here\" in a gray box labeled \"Chat Bot\" on the far left. On the right side of the input area, there's a blue \"Generate More\" button.\n",
      "\n",
      "The layout is simple, with a black background and primarily white text. Some elements of the interface, like menu options, are highlighted in blue for emphasis.\n",
      "\n",
      "This screenshot could be useful for developers or тестировщики looking to test and refine the AI chatbot's functionality. The stuttering responses and amidst-effect perception might indicate issues that need to be addressed for improved user experience and accurate AI interactions."
     ]
    }
   ],
   "source": [
    "test_messages = [\n",
    "    {\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": [\n",
    "\t\t\t{\n",
    "\t\t\t\t\"type\": \"text\",\n",
    "\t\t\t\t\"text\": \"do some calculus for me\"\n",
    "\t\t\t}\n",
    "\t\t]\n",
    "\t}\n",
    "]\n",
    "\n",
    "test_messages_multi = [\n",
    "    {\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": [\n",
    "\t\t\t{\n",
    "\t\t\t\t\"type\": \"text\",\n",
    "\t\t\t\t\"text\": \"Describe this image to me:\"\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"type\": \"image_url\",\n",
    "\t\t\t\t\"image_url\": {\n",
    "\t\t\t\t\t\"url\": \"https://preview.redd.it/vb1h8ahxlzud1.png?width=1165&format=png&auto=webp&s=b9e929f358f94150cf79f0ed4577d591f1e652eb\"\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t],\n",
    "\t}\n",
    "]\n",
    "\n",
    "\n",
    "generator = deployment.get_result_loop({\n",
    "\t\"chat_history\": test_messages_multi,\n",
    "\t# \"question\": \"Describe this image to me:\",\n",
    "\t\"max_tokens\": 1024,\n",
    "\t\"temperature\": 0.5,\n",
    "\t\"top_k\": 20,\n",
    "})\n",
    "\n",
    "num_returned, tokens_generated = 0, 0\n",
    "\n",
    "async for result in generator:\n",
    "\ttokens_generated += 1\n",
    "\ttext_outputs = [output.text for output in result.outputs]\n",
    "\tassert len(text_outputs) == 1\n",
    "\ttext_output = text_outputs[0][num_returned:]\n",
    "\tnum_returned = num_returned + len(text_output)\n",
    "    \n",
    "\ttext_outputs = [output.text for output in result.outputs]\n",
    "\t# print(len(text_outputs))\t\n",
    "\t\n",
    "\tprint(text_output, end=\"\")\n",
    "\tpass\n",
    "\n",
    "# print(safe_serialize(result, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QL_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
